{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p><code>ZOO WES Runner</code> provides a client library to connect the ZOO Project to an HPC using toil Workflow Execution Service (WES).</p> <p>The goal is to ease the development of runners that implement a business logic for the EOEPCA ADES ZOO-Project implementation.</p> <p>A runner provides an execution engine for ZOO-Project. This repository and documentation provides a runner for toil WES.</p> <p>Below an overview of the building block</p> <p></p>"},{"location":"#service-deployment","title":"Service deployment","text":"<p>When a service is deployed, the ADES instantiates a cookiecutter processing service template.</p> <p>The scaffolded service folder contains a <code>service.py</code> Python file that executes the Application Package.</p> <p>The <code>service.py</code> must implement a function with the signature:</p> <pre><code>def {{cookiecutter.workflow_id | replace(\"-\", \"_\")  }}(conf, inputs, outputs):\n</code></pre> <p>And return <code>zoo.SERVICE_SUCCEEDED</code> if the execution is a success or <code>zoo.SERVICE_FAILED</code> if failed.</p> <p>It must also implement an <code>ExecutionHandler</code>.</p> <p>The <code>ExecutionHandler</code> is a abstract class defined as follows:</p> <pre><code>from abc import ABC, abstractmethod\n\n\nclass ExecutionHandler(ABC):\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n        self.job_id = None\n\n    def set_job_id(self, job_id):\n        self.job_id = job_id\n\n    @abstractmethod\n    def pre_execution_hook(self):\n        pass\n\n    @abstractmethod\n    def post_execution_hook(self):\n        pass\n\n    @abstractmethod\n    def get_secrets(self):\n        pass\n\n    @abstractmethod\n    def get_pod_env_vars(self):\n        pass\n\n    @abstractmethod\n    def get_pod_node_selector(self):\n        pass\n\n    @abstractmethod\n    def handle_outputs(self, execution_log, output, usage_report, tool_logs=None):\n        pass\n\n    @abstractmethod\n    def get_additional_parameters(self):\n        pass\n</code></pre>"},{"location":"#service-execution","title":"Service execution","text":"<p>The service execution follows the <code>ZooWESRunner</code> execution defined in its <code>execute</code> method.</p>"},{"location":"#what-eoepca-provides","title":"What EOEPCA provides","text":"<p>EOEPCA provides:</p> <ul> <li>a example of a ZOO-Service template in the https://github.com/EOEPCA/eoepca-proc-service-template-wes software repository</li> </ul> <p>Other service template can of course be implemented with different business logics and interfacing with other systems or APIs.</p>"},{"location":"01_configuring_toil/","title":"toil settings","text":""},{"location":"01_configuring_toil/#prerequisites","title":"Prerequisites","text":"<ul> <li>A user account which is able to submit jobs into SLURM. In the examples, this is <code>eoepca-toil</code>.</li> <li>Shared storage which is mounted on all the SLURM nodes. In the examples, this is <code>/opt/toil/toil_storage</code>.</li> </ul>"},{"location":"01_configuring_toil/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"01_configuring_toil/#install-toil","title":"Install TOIL","text":"<ul> <li>Make directories for toil: <code>mkdir -p /opt/toil/toil_storage &amp;&amp; cd /opt/toil</code></li> <li>Create a venv for the toil install <code>python3 -m venv environ &amp;&amp; cd source environ/bin/activate</code></li> <li>Install toil into the environ <code>python3 -m pip install toil[all]</code></li> </ul>"},{"location":"01_configuring_toil/#install-apptainer-formerly-singularity-on-all-the-slurm-nodes","title":"Install apptainer (formerly singularity) on all the SLURM nodes.","text":"<ul> <li><code>yum install apptainer apptainer-suid</code> on every node.</li> </ul>"},{"location":"01_configuring_toil/#checking-toil-is-working-in-your-environment","title":"Checking TOIL is working in your environment","text":"<p>Before continuing, you should check that TOIL jobs work when you submit them locally, and use this to iterate on the extra parameters you need to give to TOIL (eg default memory allocation, batch system) to get this to work on your system.</p> <p>An example set of steps which works on our system is given below:</p> <pre><code># Clone repository\ngit clone -b develop https://github.com/gfenoy/eoepca-zoo-wes-runner.git\n# Move to the right location\ncd eoepca-zoo-wes-runner\n# Create the required directories\nmkdir -p toil_storage/example/{work_dir,job_store}\n# Execute a CWL Application Package using slurm batchSystem\ntoil-cwl-runner \\\n    --batchSystem slurm \\\n    --defaultMemory 500Mi \\\n    --maxMemory 100Gi \\\n    --singularity \\\n    --workDir toil_storage/example/work_dir \\\n    --jobStore toil_storage/example/job_store/$(uuidgen) \\\n    extras/example/app-package.cwl#water_bodies \\\n    extras/example/params.yaml\n</code></pre> <p>Additional parameters you can give to toil-cwl-runner are given here</p>"},{"location":"01_configuring_toil/#configuring-and-starting-the-toil-server","title":"Configuring and starting the TOIL server","text":"<p>To start TOIL in server mode, some additional configuration is required. A number of services must be started, as described here. All of these commands are run on the same SLRUM headnode.</p>"},{"location":"01_configuring_toil/#start-a-rabbitmq-server","title":"Start a rabbitmq server","text":""},{"location":"01_configuring_toil/#with-docker","title":"With Docker","text":"<p>Toil's celery worker requires a rabbitmq broker. You cn start this with docker using the following command: <pre><code>docker run -d --restart=always \\\n--name toil-wes-rabbitmq \\\n-p 127.0.0.1:5672:5672 rabbitmq:3.9.5\n</code></pre></p>"},{"location":"01_configuring_toil/#with-apptainer","title":"With Apptainer","text":"<p>If apptainer is available but no docker, use the command below: <pre><code>apptainer run \\\n--bind $(pwd)/etc:/opt/bitnami/rabbitmq/etc/rabbitmq/ \\\n--bind $(pwd)/var_lib:/opt/bitnami/rabbitmq/var/lib/rabbitmq \\\n--bind $(pwd)/.rabbitmq:/opt/bitnami/rabbitmq/.rabbitmq/ \\\n--bind $(pwd)/tmp:/bitnami/rabbitmq/ \\\ndocker://bitnami/rabbitmq:latest\n</code></pre></p>"},{"location":"01_configuring_toil/#start-toils-celery-worker","title":"Start TOIL's Celery worker","text":""},{"location":"01_configuring_toil/#without-systemd","title":"Without systemd","text":"<ul> <li>Edit the file venv3.11/lib/python3.11/site-packages/toil/server/celery_app.py and replace user as username and password with the one you defined in the rabbitmq server (per default, you can use <code>user</code> as username and <code>bitnami</code> as password).</li> </ul> <pre><code>source ~/venv3.11/bin/activate\ncelery --broker=amqp://user:bitnami@127.0.0.1:5672// \\\n-A toil.server.celery_app worker \\\n--loglevel=INFO\n</code></pre>"},{"location":"01_configuring_toil/#with-systemd","title":"With systemd","text":"<p>This will be started using a systemd unit.</p> <ul> <li>Copy <code>extras/toil-celery.service</code> to <code>/etc/systemd/system</code></li> <li>Configure this file to run as the appropriate user, and to call toil from the appropriate location if you are not using the location above.</li> <li><code>systemctl daemon-reload &amp;&amp; systemctl start toil-celery &amp;&amp; systemctl enable toil-celery</code></li> </ul>"},{"location":"01_configuring_toil/#start-the-toil-wes-server","title":"Start the TOIL WES Server","text":""},{"location":"01_configuring_toil/#without-systemd_1","title":"Without systemd","text":"<p>The WES server can be started with the following command:</p> <pre><code>source ~/venv3.11/bin/activate TOIL_WES_BROKER_URL=amqp://user:bitnami@127.0.0.1:5672//  toil server \\\n--opt=--batchSystem=slurm \\\n--opt=--defaultMemory=500Mi \\\n--opt=--maxMemory=100Gi \\\n--opt=--singularity </code></pre> <p>With these options, we ensure that the singularity container will be executed through SLURM.</p>"},{"location":"01_configuring_toil/#with-systemd_1","title":"With systemd","text":"<p>This will be started using a systemd unit.</p> <ul> <li>Copy <code>extras/toil-server.service</code> to <code>/etc/systemd/system</code></li> <li>Configure this file to run as the appropriate user, and to call toil from the appropriate location if you are not using the location above.</li> <li>You should adjust the <code>--opt</code> options to those which were required to run TOIL in the \"Checking TOIL is working in your environment\" step above, and you can add other options if you wish.</li> <li><code>systemctl daemon-reload &amp;&amp; systemctl start toil-server &amp;&amp; systemctl enable toil-server</code></li> </ul>"},{"location":"01_configuring_toil/#reverse-proxy-the-wes-api-using-ngix","title":"Reverse proxy the WES API Using ngix.","text":"<p>The TOIL WES API is exposed through a reverse proxy, which allows us to add basic authentication to the API.</p> <ul> <li><code>yum install nginx &amp;&amp; cd /etc/nginx</code></li> <li>Create a basic auth user and password for toil: <code>htpasswd ./htpasswd toil-username</code> and make a password. Make sure you note the password down.</li> <li>Configure nginx using the configuration snippet in <code>extras/nginx.conf</code>. You should makse sure https is used to secure the authentication.</li> <li>Expose nginx port you configured abover (probably 443) through the firewall.</li> </ul>"},{"location":"02_configuring_ades/","title":"Network settings","text":""},{"location":"02_configuring_ades/#configuring-the-zoo-project-dru","title":"Configuring the ZOO-Project-DRU.","text":"<p>The ZOO-Project-DRU must be configured to do the following:</p> <ul> <li>Run an ZOO-Project docker image which has the python packaged contained in this repo installed.</li> <li>Use the cookiecutter from here instead of the one for calrissian.</li> <li>Pass the URL and basic auth information as environment variables into the image.</li> </ul> <p>A minimal example of the helm values required to do this is contained in <code>examples/zoo-dru-values.yaml</code>. These are indended to be applied to the zoo-dru helm chart here- you will likely need to customise the other values to match your setup and environment.</p> <p>Once you have a complete values file, this can be installed into your kubernetes cluster with the following command:</p> <pre><code>helm upgrade --install \\\n    --create-namespace --namespace zoo zoo-project-dru \\\n    zoo-project/zoo-project-dru \\\n    --version 0.1.1 \\\n    --values ./values.yaml\n</code></pre>"},{"location":"02_configuring_ades/#create-ssh-tunnels-optional","title":"Create SSH tunnels (optional)","text":"<p>We use OpenSSH tunnels to provide remote access from the Kubernetes cluster to the HPC and vice versa.</p> <p>First, we must use the following command to access the remote WES running on an identified login node (e.g., login02).</p> <pre><code># Access to WES\nssh -i myOpenSSH.key \\\n    -L 0.0.0.0:8100:127.0.0.1:8080 \\\n    &lt;user&gt;@login02.hpc.ressources.name\n</code></pre> <p>We also need to provide access to the S3 bucket on our cluster from the HPC. This bucket stores the execution's final results.</p> <pre><code># Access to S3 bukect\nssh -i myOpenSSH.key \\\n    -R 0.0.0.0:4900:127.0.0.1:4566 \\\n    &lt;user&gt;@login02.hpc.ressources.name\n</code></pre> <p>Some HPC login nodes do not allow opening ports on any IP address. Consequently, using the previous command will open the port in LISTEN mode only on the loopback address (127.0.0.1), which is unreachable from the SBATCH environment. In such cases, we should use a public address to access the S3 bucket.</p>"},{"location":"03_deploying_and_monitoring_jobs/","title":"03 deploying and monitoring jobs","text":""},{"location":"03_deploying_and_monitoring_jobs/#monitoring","title":"Monitoring","text":"<ul> <li>You can call zoo's <code>/&lt;user&gt;/ogc-api/jobs/&lt;job-id&gt;</code> endpoint to find out the status of your job, if it succeeded or failed.</li> </ul>"},{"location":"03_deploying_and_monitoring_jobs/#debugging","title":"Debugging","text":"<ul> <li>Logs are stored in the ADES zoo-fpm pod, in the conainter called zoofpm.</li> <li>Once a workflow is deployed, the cookiecutter will be run and the output stored in <code>/opt/zooservice_user</code></li> <li> <p>When a workflow is run, python errors and output are available at <code>/var/ww/html/temp</code>, which two files created for each run, named for the job ID. There is an error log, and a json status file.</p> </li> <li> <p>To check TOIL has recieved the job, you can look at the TOIL logs. If you have followed the configuration instructions in this directory, you can look at these using <code>journalctl -u toil-server.service</code> on the TOIL machine.</p> </li> <li>It is sometimes also useful to look in TOIL's server working directoty, where job status is kept.</li> <li>If you are running the jobs on SLURM, you can watch the SLURM queue to check jobs are created successfully.</li> </ul>"},{"location":"configuration/","title":"Configuration","text":""},{"location":"configuration/#environment-variables","title":"Environment variables","text":""},{"location":"configuration/#wes","title":"WES","text":"<p>WES stands for Workflow Execution Service offered by toil.</p> <ul> <li><code>WES_URL</code>: the toil WES URL</li> <li><code>WES_USER</code>: the user name to authenticate to the WES</li> <li><code>WES_PASSWORD</code>: the password to authenticate to the WES</li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>The package is installed with:</p> <pre><code>git clone -b develop https://github.com/ZOO-Project/zoo-wes-runner.git\ncd zoo-wes-runner\npoetry build\npip install dist/zoo_wes_runner-0.1.0-py3-none-any.whl\n</code></pre> <p>Test the installation with:</p> <pre><code>from zoo_wes_runner import ZooWESRunner\n</code></pre>"},{"location":"started/","title":"Getting started","text":"<p>Check the complete example provided under the folder tests/dnbr</p>"},{"location":"api/zoo_wes_runner/","title":"Module zoo_wes_runner","text":""},{"location":"api/zoo_wes_runner/#sub-modules","title":"Sub-modules","text":"<ul> <li>zoo_wes_runner.base</li> <li>zoo_wes_runner.wes_runner</li> </ul>"},{"location":"api/zoo_wes_runner/#classes","title":"Classes","text":""},{"location":"api/zoo_wes_runner/#zoowesrunner","title":"ZooWESRunner","text":"<pre><code>class ZooWESRunner(\n    *args,\n    **kwargs\n)\n</code></pre> <p>We wrap the base zoo runner but add our own execution step.</p>"},{"location":"api/zoo_wes_runner/#ancestors-in-mro","title":"Ancestors (in MRO)","text":"<ul> <li>zoo_wes_runner.base.BaseZooRunner</li> <li>zoo_calrissian_runner.ZooCalrissianRunner</li> </ul>"},{"location":"api/zoo_wes_runner/#static-methods","title":"Static methods","text":""},{"location":"api/zoo_wes_runner/#shorten_namespace","title":"shorten_namespace","text":"<pre><code>def shorten_namespace(\n    value: str\n) -&gt; str\n</code></pre> <p>shortens the namespace to 63 characters</p>"},{"location":"api/zoo_wes_runner/#methods","title":"Methods","text":""},{"location":"api/zoo_wes_runner/#assert_parameters","title":"assert_parameters","text":"<pre><code>def assert_parameters(\n    self\n)\n</code></pre> <p>checks all mandatory processing parameters were provided</p>"},{"location":"api/zoo_wes_runner/#dismiss","title":"dismiss","text":"<pre><code>def dismiss(\n    self\n)\n</code></pre> <p>Cancel the job.</p>"},{"location":"api/zoo_wes_runner/#execute","title":"execute","text":"<pre><code>def execute(\n    self\n)\n</code></pre> <p>Execute some CWL on a WES Server.</p>"},{"location":"api/zoo_wes_runner/#get_max_cores","title":"get_max_cores","text":"<pre><code>def get_max_cores(\n    self\n) -&gt; int\n</code></pre> <p>returns the maximum number of cores that pods can use</p>"},{"location":"api/zoo_wes_runner/#get_max_ram","title":"get_max_ram","text":"<pre><code>def get_max_ram(\n    self\n) -&gt; str\n</code></pre> <p>returns the maximum RAM that pods can use</p>"},{"location":"api/zoo_wes_runner/#get_namespace_name","title":"get_namespace_name","text":"<pre><code>def get_namespace_name(\n    self\n)\n</code></pre> <p>creates or returns the namespace</p>"},{"location":"api/zoo_wes_runner/#get_processing_parameters","title":"get_processing_parameters","text":"<pre><code>def get_processing_parameters(\n    self\n)\n</code></pre> <p>Gets the processing parameters from the zoo inputs</p>"},{"location":"api/zoo_wes_runner/#get_volume_size","title":"get_volume_size","text":"<pre><code>def get_volume_size(\n    self\n) -&gt; str\n</code></pre> <p>returns volume size that the pods share</p>"},{"location":"api/zoo_wes_runner/#get_workflow_id","title":"get_workflow_id","text":"<pre><code>def get_workflow_id(\n    self\n)\n</code></pre> <p>returns the workflow id (CWL entry point)</p>"},{"location":"api/zoo_wes_runner/#get_workflow_inputs","title":"get_workflow_inputs","text":"<pre><code>def get_workflow_inputs(\n    self,\n    mandatory=False\n)\n</code></pre> <p>Returns the CWL workflow inputs</p>"},{"location":"api/zoo_wes_runner/#prepare","title":"prepare","text":"<pre><code>def prepare(\n    self\n)\n</code></pre> <p>Generic pre-execution which applies to all handlers.</p>"},{"location":"api/zoo_wes_runner/#update_status","title":"update_status","text":"<pre><code>def update_status(\n    self,\n    progress: int,\n    message: str = None\n) -&gt; None\n</code></pre> <p>updates the execution progress (%) and provides an optional message</p>"},{"location":"api/zoo_wes_runner/#wrap","title":"wrap","text":"<pre><code>def wrap(\n    self\n)\n</code></pre>"},{"location":"api/zoo_wes_runner/base/","title":"Module zoo_wes_runner.base","text":"<p>Bases classes for Zoo runners.</p> <p>These are derived here from the zoo-calrissian-runner because no generic abstract classes exist.</p>"},{"location":"api/zoo_wes_runner/base/#variables","title":"Variables","text":"<pre><code>logger\n</code></pre>"},{"location":"api/zoo_wes_runner/base/#classes","title":"Classes","text":""},{"location":"api/zoo_wes_runner/base/#basezoorunner","title":"BaseZooRunner","text":"<pre><code>class BaseZooRunner(\n    cwl,\n    conf,\n    inputs,\n    outputs,\n    execution_handler: Optional[zoo_calrissian_runner.handlers.ExecutionHandler] = None\n)\n</code></pre> <p>Mangle the ZooCalrissianRunner to be a base class to inherit from.</p>"},{"location":"api/zoo_wes_runner/base/#ancestors-in-mro","title":"Ancestors (in MRO)","text":"<ul> <li>zoo_calrissian_runner.ZooCalrissianRunner</li> </ul>"},{"location":"api/zoo_wes_runner/base/#descendants","title":"Descendants","text":"<ul> <li>zoo_wes_runner.wes_runner.ZooWESRunner</li> </ul>"},{"location":"api/zoo_wes_runner/base/#static-methods","title":"Static methods","text":""},{"location":"api/zoo_wes_runner/base/#shorten_namespace","title":"shorten_namespace","text":"<pre><code>def shorten_namespace(\n    value: str\n) -&gt; str\n</code></pre> <p>shortens the namespace to 63 characters</p>"},{"location":"api/zoo_wes_runner/base/#methods","title":"Methods","text":""},{"location":"api/zoo_wes_runner/base/#assert_parameters","title":"assert_parameters","text":"<pre><code>def assert_parameters(\n    self\n)\n</code></pre> <p>checks all mandatory processing parameters were provided</p>"},{"location":"api/zoo_wes_runner/base/#execute","title":"execute","text":"<pre><code>def execute(\n    self\n)\n</code></pre> <p>This function should be implmented to provide job exection logic.</p>"},{"location":"api/zoo_wes_runner/base/#get_max_cores","title":"get_max_cores","text":"<pre><code>def get_max_cores(\n    self\n) -&gt; int\n</code></pre> <p>returns the maximum number of cores that pods can use</p>"},{"location":"api/zoo_wes_runner/base/#get_max_ram","title":"get_max_ram","text":"<pre><code>def get_max_ram(\n    self\n) -&gt; str\n</code></pre> <p>returns the maximum RAM that pods can use</p>"},{"location":"api/zoo_wes_runner/base/#get_namespace_name","title":"get_namespace_name","text":"<pre><code>def get_namespace_name(\n    self\n)\n</code></pre> <p>creates or returns the namespace</p>"},{"location":"api/zoo_wes_runner/base/#get_processing_parameters","title":"get_processing_parameters","text":"<pre><code>def get_processing_parameters(\n    self\n)\n</code></pre> <p>Gets the processing parameters from the zoo inputs</p>"},{"location":"api/zoo_wes_runner/base/#get_volume_size","title":"get_volume_size","text":"<pre><code>def get_volume_size(\n    self\n) -&gt; str\n</code></pre> <p>returns volume size that the pods share</p>"},{"location":"api/zoo_wes_runner/base/#get_workflow_id","title":"get_workflow_id","text":"<pre><code>def get_workflow_id(\n    self\n)\n</code></pre> <p>returns the workflow id (CWL entry point)</p>"},{"location":"api/zoo_wes_runner/base/#get_workflow_inputs","title":"get_workflow_inputs","text":"<pre><code>def get_workflow_inputs(\n    self,\n    mandatory=False\n)\n</code></pre> <p>Returns the CWL workflow inputs</p>"},{"location":"api/zoo_wes_runner/base/#prepare","title":"prepare","text":"<pre><code>def prepare(\n    self\n)\n</code></pre> <p>Generic pre-execution which applies to all handlers.</p>"},{"location":"api/zoo_wes_runner/base/#update_status","title":"update_status","text":"<pre><code>def update_status(\n    self,\n    progress: int,\n    message: str = None\n) -&gt; None\n</code></pre> <p>updates the execution progress (%) and provides an optional message</p>"},{"location":"api/zoo_wes_runner/base/#wrap","title":"wrap","text":"<pre><code>def wrap(\n    self\n)\n</code></pre>"},{"location":"api/zoo_wes_runner/wes_runner/","title":"Module zoo_wes_runner.wes_runner","text":""},{"location":"api/zoo_wes_runner/wes_runner/#variables","title":"Variables","text":"<pre><code>logger\n</code></pre>"},{"location":"api/zoo_wes_runner/wes_runner/#classes","title":"Classes","text":""},{"location":"api/zoo_wes_runner/wes_runner/#zoowesrunner","title":"ZooWESRunner","text":"<pre><code>class ZooWESRunner(\n    *args,\n    **kwargs\n)\n</code></pre> <p>We wrap the base zoo runner but add our own execution step.</p>"},{"location":"api/zoo_wes_runner/wes_runner/#ancestors-in-mro","title":"Ancestors (in MRO)","text":"<ul> <li>zoo_wes_runner.base.BaseZooRunner</li> <li>zoo_calrissian_runner.ZooCalrissianRunner</li> </ul>"},{"location":"api/zoo_wes_runner/wes_runner/#static-methods","title":"Static methods","text":""},{"location":"api/zoo_wes_runner/wes_runner/#shorten_namespace","title":"shorten_namespace","text":"<pre><code>def shorten_namespace(\n    value: str\n) -&gt; str\n</code></pre> <p>shortens the namespace to 63 characters</p>"},{"location":"api/zoo_wes_runner/wes_runner/#methods","title":"Methods","text":""},{"location":"api/zoo_wes_runner/wes_runner/#assert_parameters","title":"assert_parameters","text":"<pre><code>def assert_parameters(\n    self\n)\n</code></pre> <p>checks all mandatory processing parameters were provided</p>"},{"location":"api/zoo_wes_runner/wes_runner/#dismiss","title":"dismiss","text":"<pre><code>def dismiss(\n    self\n)\n</code></pre> <p>Cancel the job.</p>"},{"location":"api/zoo_wes_runner/wes_runner/#execute","title":"execute","text":"<pre><code>def execute(\n    self\n)\n</code></pre> <p>Execute some CWL on a WES Server.</p>"},{"location":"api/zoo_wes_runner/wes_runner/#get_max_cores","title":"get_max_cores","text":"<pre><code>def get_max_cores(\n    self\n) -&gt; int\n</code></pre> <p>returns the maximum number of cores that pods can use</p>"},{"location":"api/zoo_wes_runner/wes_runner/#get_max_ram","title":"get_max_ram","text":"<pre><code>def get_max_ram(\n    self\n) -&gt; str\n</code></pre> <p>returns the maximum RAM that pods can use</p>"},{"location":"api/zoo_wes_runner/wes_runner/#get_namespace_name","title":"get_namespace_name","text":"<pre><code>def get_namespace_name(\n    self\n)\n</code></pre> <p>creates or returns the namespace</p>"},{"location":"api/zoo_wes_runner/wes_runner/#get_processing_parameters","title":"get_processing_parameters","text":"<pre><code>def get_processing_parameters(\n    self\n)\n</code></pre> <p>Gets the processing parameters from the zoo inputs</p>"},{"location":"api/zoo_wes_runner/wes_runner/#get_volume_size","title":"get_volume_size","text":"<pre><code>def get_volume_size(\n    self\n) -&gt; str\n</code></pre> <p>returns volume size that the pods share</p>"},{"location":"api/zoo_wes_runner/wes_runner/#get_workflow_id","title":"get_workflow_id","text":"<pre><code>def get_workflow_id(\n    self\n)\n</code></pre> <p>returns the workflow id (CWL entry point)</p>"},{"location":"api/zoo_wes_runner/wes_runner/#get_workflow_inputs","title":"get_workflow_inputs","text":"<pre><code>def get_workflow_inputs(\n    self,\n    mandatory=False\n)\n</code></pre> <p>Returns the CWL workflow inputs</p>"},{"location":"api/zoo_wes_runner/wes_runner/#prepare","title":"prepare","text":"<pre><code>def prepare(\n    self\n)\n</code></pre> <p>Generic pre-execution which applies to all handlers.</p>"},{"location":"api/zoo_wes_runner/wes_runner/#update_status","title":"update_status","text":"<pre><code>def update_status(\n    self,\n    progress: int,\n    message: str = None\n) -&gt; None\n</code></pre> <p>updates the execution progress (%) and provides an optional message</p>"},{"location":"api/zoo_wes_runner/wes_runner/#wrap","title":"wrap","text":"<pre><code>def wrap(\n    self\n)\n</code></pre>"}]}